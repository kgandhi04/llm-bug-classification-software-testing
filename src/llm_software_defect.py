# -*- coding: utf-8 -*-
"""llm-software-defect.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rTb_RikwJZuwCrcfy-VEUHX1rXUOkm0r
"""

from google.colab import drive
# Mount Google Drive to Colab
drive.mount('/content/drive')

!pip install transformers
!pip install datasets
!pip install wandb
!pip install simpletransformers
!pip install git+https://github.com/huggingface/transformers



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import re
import unicodedata
import gensim
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from huggingface_hub import login
from transformers import EarlyStoppingCallback

import os

# Define the path where you want to save models and results in Google Drive
drive_folder = "/content/drive/My Drive/Colab Notebooks/llm-software-quality/models/"

# Create the directory if it doesn't exist
os.makedirs(drive_folder, exist_ok=True)

from gensim.parsing.preprocessing import remove_stopwords

# Define regex patterns for text preprocessing
function_sig_regex = re.compile(r'[a-zA-Z][a-zA-Z0-9_.]*\([a-zA-Z0-9_, ]*\)')
issue_id_regex = re.compile(r'#[0-9]+')
non_ascii_char_regex = re.compile(r'[^\x00-\x7f]')
punctuations = r'!\'"`$%&\()*,/:;<=>[\\]^{|}~+#@-_'
punctuations_trans = str.maketrans(punctuations, " " * len(punctuations))

# Text preprocessing function
def preprocess_text(text, max_tokens=None):
    text = str(text).lower()  # Convert to lowercase
    text = function_sig_regex.sub(" FUNCTION ", text)  # Replace function signatures
    text = issue_id_regex.sub(" ISSUE ", text)  # Replace issue IDs
    text = text.translate(punctuations_trans)  # Remove punctuations
    text = non_ascii_char_regex.sub("", text)  # Remove non-ASCII characters
    text = unicodedata.normalize('NFD', text)  # Normalize Unicode characters
    text = gensim.parsing.preprocessing.strip_multiple_whitespaces(text)  # Remove multiple spaces
    text = remove_stopwords(text)  # Remove stopwords

    if max_tokens is not None:
        text = " ".join(text.split()[:max_tokens])  # Limit to max tokens

    return text
# Hugging Face login
print("Logging into Hugging Face...")
hf_token = "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"
login(token=hf_token)
print("Login successful.")

# File path example inside Google Drive
train_dataset_path = '/content/drive/My Drive/Colab Notebooks/llm-software-quality/datafiles/train.csv'
test_dataset_path = '/content/drive/My Drive/Colab Notebooks/llm-software-quality/datafiles/test.csv'

train_df = pd.read_csv(train_dataset_path)
test_df = pd.read_csv(test_dataset_path)
# train_df=train_df.head(100000)
# test_df=test_df.head(10000)
print("Datasets loaded successfully.")

# Combine 'title' and 'body' and preprocess text
train_df['text'] = (train_df['title'] + " " + train_df['body']).apply(preprocess_text)

# Encode labels
label_encoder = LabelEncoder()
train_df['label_encoded'] = label_encoder.fit_transform(train_df['labels'])

# Sample 25,000 instances per label
train_df = train_df.groupby('label_encoded', group_keys=False).apply(lambda x: x.sample(min(len(x), 25000), random_state=42)).reset_index(drop=True)

# Split data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_df['text'], train_df['label_encoded'], test_size=0.2, random_state=42, stratify=train_df['label_encoded']
)

# Define dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.encodings = tokenizer(texts.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')
        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item

# Define models
models = {
    "BERT": "google-bert/bert-base-uncased",
    "ModernBERT": "answerdotai/ModernBERT-base",
    "RoBERTa": "FacebookAI/roberta-base"
}

# Train and evaluate each model
results = {}
for model_name, model_path in models.items():
    print(f"\nTraining {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)

    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(label_encoder.classes_))

    training_args = TrainingArguments(
        output_dir=f'./results_{model_name}',
        evaluation_strategy='epoch',
        learning_rate=1e-5,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        num_train_epochs=10,
        weight_decay=0.01,
        logging_dir=f'./logs_{model_name}',
        report_to='none',
        save_strategy='epoch',
        save_total_limit=3,
        logging_first_step=True,
        logging_steps=1,
        metric_for_best_model='eval_loss',
        load_best_model_at_end=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    trainer.train()

    # Save the trained model and tokenizer to Google Drive
    model_save_path = os.path.join(drive_folder, f"{model_name}_model")
    tokenizer_save_path = os.path.join(drive_folder, f"{model_name}_tokenizer")

    trainer.save_model(model_save_path)
    tokenizer.save_pretrained(tokenizer_save_path)

    print(f"Model {model_name} saved to {model_save_path}.")

    predictions = trainer.predict(val_dataset)
    y_preds = torch.argmax(torch.tensor(predictions.predictions), dim=1).numpy()

    # Save results
    report = classification_report(val_labels, y_preds, target_names=label_encoder.classes_, output_dict=True)
    results[model_name] = report

    # Confusion Matrix
    conf_matrix = confusion_matrix(val_labels, y_preds)
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.savefig(f'conf_matrix_{model_name}.png')
    plt.close()

# Train FastText Model
print("\nTraining FastText Model...")
vectorizer = TfidfVectorizer(max_features=10000)
X_train_tfidf = vectorizer.fit_transform(train_texts)
X_val_tfidf = vectorizer.transform(val_texts)

fasttext_model = LogisticRegression(max_iter=500)
fasttext_model.fit(X_train_tfidf, train_labels)
y_preds_fasttext = fasttext_model.predict(X_val_tfidf)

# Save results for FastText
report_fasttext = classification_report(val_labels, y_preds_fasttext, target_names=label_encoder.classes_, output_dict=True)
results["FastText"] = report_fasttext

# Confusion Matrix for FastText
conf_matrix_fasttext = confusion_matrix(val_labels, y_preds_fasttext)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix_fasttext, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - FastText')
plt.savefig('conf_matrix_FastText.png')
plt.close()

# Save results
joblib.dump(results, "model_evaluation_results.pkl")

print("All models trained and evaluated successfully.")

import joblib
import shutil

# Save FastText Model and TF-IDF Vectorizer
joblib.dump(fasttext_model, os.path.join(drive_folder, "fasttext_model.pkl"))
joblib.dump(vectorizer, os.path.join(drive_folder, "tfidf_vectorizer.pkl"))

print(f"FastText model saved at {drive_folder}.")

# Save evaluation results
joblib.dump(results, os.path.join(drive_folder, "model_evaluation_results.pkl"))

# Save confusion matrices
for model_name in models.keys():
    conf_matrix_path = f'conf_matrix_{model_name}.png'
    shutil.move(conf_matrix_path, os.path.join(drive_folder, conf_matrix_path))

shutil.move('conf_matrix_FastText.png', os.path.join(drive_folder, 'conf_matrix_FastText.png'))

print("All models, evaluation reports, and confusion matrices saved successfully in Google Drive.")